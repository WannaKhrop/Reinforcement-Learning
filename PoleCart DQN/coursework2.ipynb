{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "62N9KGkTKQRO"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym.core import Env\n",
        "from collections import deque\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from utils import DQN, ReplayBuffer, greedy_action, epsilon_greedy, update_target, loss, test_agent, fit_agent, plot_data_for_states, eps_sequence, get_n_episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y-9WDFtLbmy",
        "outputId": "3d293429-ae1f-4c8b-dca8-87a00219e26a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of episodes in runs =  600\n"
          ]
        }
      ],
      "source": [
        "NUM_RUNS = 10\n",
        "BASE_EPS = 0.995\n",
        "N_EPISODES = get_n_episodes(BASE_EPS)\n",
        "\n",
        "print(\"Number of episodes in runs = \", N_EPISODES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oFUZvWYzfoz4"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 3000\n",
        "BATCH_SIZE = 35\n",
        "COLLECTING_STEPS = 400\n",
        "TARGET_UPDATE_FREQUENCY = 80\n",
        "GAMMA = 1.0\n",
        "DELAY = 100\n",
        "LAYERS = [4, 15, 2]\n",
        "ACTIVATIONS = [F.tanh, F.leaky_relu]\n",
        "LR = 0.001\n",
        "\n",
        "P_SHUFFLE = 0.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "id": "TTlIkU9ULeDd",
        "outputId": "a4080ba1-6f16-4e93-a899-269485b3b74d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 1 of 10\n",
            "Progress: 0% 10% 20% "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c599e36e7100>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Run {run + 1} of {NUM_RUNS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   agent, buffer, episode_durations, loss_progress, results_of_test = fit_agent(env=env,\n\u001b[0m\u001b[1;32m     12\u001b[0m                                                                                \u001b[0mbase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBASE_EPS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                                                                \u001b[0mdelay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDELAY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mfit_agent\u001b[0;34m(env, base, delay, buffer_size, p_shuffle, batch_size, collecting_steps, update_frequency, gamma, layers, activations, lr)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m           \u001b[0;31m# Select an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m           \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m           \u001b[0;31m# Perform an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mepsilon_greedy\u001b[0;34m(epsilon, dqn, state)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m# get best actions ==> mask has True on the best actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2791\u001b[0m     \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2792\u001b[0m     \"\"\"\n\u001b[0;32m-> 2793\u001b[0;31m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[1;32m   2794\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   2795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "runs_results = []\n",
        "loss_results = []\n",
        "performance_results = []\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "\n",
        "for run in range(NUM_RUNS):\n",
        "  print(f\"Run {run + 1} of {NUM_RUNS}\")\n",
        "\n",
        "  agent, buffer, episode_durations, loss_progress, results_of_test = fit_agent(env=env,\n",
        "                                                                               base=BASE_EPS,\n",
        "                                                                               delay=DELAY,\n",
        "                                                                               buffer_size=BUFFER_SIZE,\n",
        "                                                                               p_shuffle=P_SHUFFLE,\n",
        "                                                                               batch_size=BATCH_SIZE,\n",
        "                                                                               collecting_steps=COLLECTING_STEPS,\n",
        "                                                                               update_frequency=TARGET_UPDATE_FREQUENCY,\n",
        "                                                                               gamma=GAMMA,\n",
        "                                                                               layers=LAYERS,\n",
        "                                                                               activations=ACTIVATIONS,\n",
        "                                                                               lr=LR)\n",
        "  performance_results.append(results_of_test)\n",
        "  runs_results.append(episode_durations)\n",
        "  loss_results.append(loss_progress)\n",
        "\n",
        "print('Complete')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPd6U5sSQal8"
      },
      "outputs": [],
      "source": [
        "# process the results\n",
        "reward_results = np.array(runs_results)\n",
        "\n",
        "mean_duration = np.mean(reward_results, axis=0)\n",
        "std_duration = np.std(reward_results, axis=0)\n",
        "\n",
        "upper_bound = mean_duration + std_duration\n",
        "lower_bound = mean_duration - std_duration\n",
        "lower_bound[lower_bound <= 0] = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuapeOQKZuYy"
      },
      "outputs": [],
      "source": [
        "x_axis_length = len(runs_results[0]) + 1\n",
        "x_axis = np.arange(1, x_axis_length)\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.plot(x_axis, mean_duration, color='r', label='Mean number of steps')\n",
        "plt.fill_between(x_axis, lower_bound, upper_bound, alpha=0.4, color='gray', label='STD in number of steps')\n",
        "plt.xlabel('Number of episodes')\n",
        "plt.ylabel('Livetime of the cart')\n",
        "plt.legend()\n",
        "plt.title(label='Learning progress for DQN', loc='center')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "performance_results = np.array(performance_results).reshape(-1)\n",
        "\n",
        "# This histogramm gives more-less clear picture about performance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.title(label='Results of learning', loc='center')\n",
        "plt.hist(performance_results, bins=50, color='c', edgecolor='k')\n",
        "\n",
        "# mean + std\n",
        "plt.axvline(np.mean(performance_results), color='r', linestyle='dashed', label='Mean Value')\n",
        "plt.axvline(np.mean(performance_results) - np.std(performance_results), color='y', linestyle='dashed', label='Standard Deviation')\n",
        "plt.axvline(np.mean(performance_results) + np.std(performance_results), color='y', linestyle='dashed')\n",
        "\n",
        "plt.xlabel('Lifetime of the pole')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(np.arange(0, 500, 25))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k4SeaAqrK8Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZNuSspFU5g9"
      },
      "outputs": [],
      "source": [
        "error_results = np.array(loss_results)\n",
        "\n",
        "mean_loss = np.mean(error_results, axis=0)\n",
        "std_loss = np.std(error_results, axis=0)\n",
        "\n",
        "upper_bound = mean_loss + std_loss\n",
        "lower_bound = mean_loss - std_loss\n",
        "lower_bound[lower_bound <= 0] = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1kKfHQoT7q5"
      },
      "outputs": [],
      "source": [
        "x_axis_length = len(runs_results[0]) + 1\n",
        "x_axis = np.arange(1, x_axis_length)\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.plot(x_axis, mean_loss, color='r', label='Mean loss over learning')\n",
        "plt.fill_between(x_axis, lower_bound, upper_bound, alpha=0.4, color='gray', label='STD in loss over learning')\n",
        "plt.xlabel('Number of episodes')\n",
        "plt.ylabel('Loss value')\n",
        "plt.legend()\n",
        "plt.title(label='Loss dynamics over learning', loc='center')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdI5dt2KXLlY"
      },
      "outputs": [],
      "source": [
        "# model for plots later\n",
        "model, _, _, _, _ = fit_agent(env=env,\n",
        "                              base=BASE_EPS,\n",
        "                              delay=DELAY,\n",
        "                              buffer_size=BUFFER_SIZE,\n",
        "                              p_shuffle=P_SHUFFLE,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              collecting_steps=COLLECTING_STEPS,\n",
        "                              update_frequency=TARGET_UPDATE_FREQUENCY,\n",
        "                              gamma=GAMMA,\n",
        "                              layers=LAYERS,\n",
        "                              activations=ACTIVATIONS,\n",
        "                              lr=LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u90IYnFiBC8A"
      },
      "outputs": [],
      "source": [
        "results_of_test = test_agent(model, env, n_runs=200)\n",
        "plt.scatter(range(len(results_of_test)), results_of_test, color='r', marker='.')\n",
        "plt.ylabel('Number of steps')\n",
        "plt.show()\n",
        "\n",
        "print('Average lifetime = ', round(np.mean(results_of_test), 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY-FhhF_Bfjo"
      },
      "outputs": [],
      "source": [
        "# data for the following plots\n",
        "N_SPLIT = 500\n",
        "\n",
        "cart_pos = 0.0\n",
        "cart_velocities = [0.0, 0.5, 1.0, 2.0]\n",
        "pole_angles = np.linspace(-0.2095, 0.2095, endpoint=True, num=N_SPLIT)\n",
        "pole_velocities = np.linspace(-5.0, 5.0, endpoint=True, num=N_SPLIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGrLojhXAvyO"
      },
      "outputs": [],
      "source": [
        "resulting_actions = {}\n",
        "resulting_q_values = {}\n",
        "\n",
        "for i, cart_velocity in enumerate([0.0, 0.5, 1.0, 2.0]):\n",
        "  angle, velocity = np.meshgrid(pole_angles, pole_velocities)\n",
        "\n",
        "  states = np.array([np.repeat(cart_pos, N_SPLIT ** 2), np.repeat(cart_velocity, N_SPLIT ** 2), angle.ravel(), velocity.ravel()]).T\n",
        "\n",
        "  batch = torch.Tensor(states)\n",
        "  results = model(batch).cpu().detach().numpy().reshape((N_SPLIT, N_SPLIT, -1))\n",
        "\n",
        "  resulting_actions[cart_velocity] = (angle, velocity, np.argmax(results, axis=2))\n",
        "  resulting_q_values[cart_velocity] = (angle, velocity, np.max(results, axis=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxjMKbxZROIU"
      },
      "outputs": [],
      "source": [
        "# plot acitons depending ob states\n",
        "plot_data_for_states(resulting_actions, title='Actions depending on states', discrete=True)\n",
        "\n",
        "# plot Q-Values dependin on states\n",
        "plot_data_for_states(resulting_q_values, title='Q-Values depending on states')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NBUQJ-mF-C6Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}